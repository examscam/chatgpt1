!pip install transformers torch
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load pre-trained model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# Initialize chat history
chat_history = None

def chat():
    global chat_history
    print("Chatbot (Type 'exit' to end)")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("Chatbot: Goodbye!")
            break
        
        # Tokenize input and append to history
        new_input = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')
        chat_history = torch.cat([chat_history, new_input], dim=-1) if chat_history is not None else new_input
        
        # Generate response
        bot_output = model.generate(chat_history, max_length=1000, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, top_p=0.9, temperature=0.7)
        
        # Decode and print response
        bot_response = tokenizer.decode(bot_output[:, chat_history.shape[-1]:][0], skip_special_tokens=True)
        print(f"Chatbot: {bot_response}")
        
        # Update chat history for next input
        chat_history = bot_output
chat()

